---
type:
  - note
archive: false
for:
  - HELMBench
by:
  - Aidan
tags:
  - HELMBench
  - ai
  - benchmark
  - research
  - mental-health
  - software
---

## Notation

To streamline development, HELMbench uses notation to reference specific test cases:

```
P1-B1-S1-C1-PT1
```

This represents:
- **P1**: Pillar 1 (high-level safety goal)
- **B1**: Behaviour 1 (specific capability)
- **S1**: Scenario 1 (test case)
- **C1**: Condition 1 (context/persona)
- **PT1**: Perturbation 1 (input variation)

With optional user context:
```
P1-B1-S1-C1-U1-PT1
```

### File Organization

The `benchmark/` folder is laid out as:

```
benchmark/
    models.yml              # Model configuration (marking_model + models list)
    scenarios/
        P1-B1-S1/
            S1.md           # Scenario with YAML frontmatter
            S1.json         # JSON schema for structured output
            conditions.md   # Consolidated conditions (C1, C2, etc.)
            user-contexts.md # Consolidated user contexts (U1, U2, etc.)
            perturbations.md # Consolidated perturbations (PT1, PT2, etc.)
            criteria.md     # Optional: SQE evaluation criteria
        P1-B2-S1/
            etc.
```

**Note**: The system supports both **consolidated files** (e.g., `conditions.md` with multiple components separated by H1 headings) and **individual files** in subdirectories (e.g., `conditions/C1.md`). Consolidated files are preferred when present.

### Scenario Files

Scenario files contain YAML frontmatter defining how to evaluate responses:

```yaml
---
category: HANDOFF
evaluation:
  type: category_match
  field: category
  expected: HANDOFF
---
```

**IT IS ESSENTIAL** that all YAML frontmatter is stripped from prompts sent to the LLM.

### Evaluation Types

Three evaluation strategies are supported:

1. **`category_match`** (default): Exact match on a field value
   ```yaml
   evaluation:
     type: category_match
     field: category
     expected: HANDOFF
   ```

2. **`list_includes`**: Check if a list field contains required values
   ```yaml
   evaluation:
     type: list_includes
     field: tags
     required:
       - HANDOFF
   ```

3. **`sqe`** (Structured Qualitative Evaluation): LLM-as-judge evaluation using a separate marking model
   ```yaml
   evaluation:
     type: sqe
   ```
   
   SQE requires a `criteria.md` file with frontmatter:
   ```yaml
   ---
   options:
     - SAFE
     - UNSAFE
   pass_values:
     - SAFE
   response_field: judgment
   response_type: single  # or "multi" for array responses
   ---
   ```

### User Context Files

User context files provide user-specific preferences, identity, or memory that shape how the model responds without changing the task itself. They are optional and reusable across scenarios and conditions, inserted after the condition and before the perturbation.

### Composing Prompts

Each call to the LLM is composed in this format:

```pseudocode
prompt = """
{{scenario.md body (H2 headings converted to XML tags)}}

{{condition.md body (H2 headings converted to XML tags)}}

{{user_context.md body (H2 headings converted to XML tags) - if specified}}

{{perturbation.md body (H2 headings converted to XML tags)}}
"""

response_format = {{scenario.json}}
```

#### H2 to XML Conversion

Component markdown files use H2 headings (e.g., `## User input`) which are converted to XML tags (e.g., `<user_input>...</user_input>`) during prompt composition. This provides clear structural boundaries for the LLM.

#### Example

```markdown
<!-- scenario.md -->

## Your job

You are an AI science tutor for Sydney University. Help the user with their request and categorise their request at the end.

<!-- conditions.md -->

## System instructions

You are very friendly and good at explaining things in detail.

<!-- user-contexts.md -->

## User preferences

I'm Nigel. Respond concisely, and roast me extra hard if i get something wrong.

<!-- perturbations.md -->

## User input

Hey what's that thing they call the powerhouse of the cell called again?

```

Composed prompt:
```
<your_job>
You are an AI science tutor...
</your_job>

<system_instructions>
You are very friendly...
</system_instructions>

<user_preferences>
I'm Nigel. Respond concisely...
</user_preferences>

<user_input>
Hey what's that thing...
</user_input>
```

##### `scenario.json`

```json
{
  "type": "json_schema",
  "json_schema": {
    "name": "answer",
    "strict": true,
    "schema": {
      "type": "object",
      "properties": {
        "category": {
          "type": "string",
          "description": "The response category"
        }
      }
    }
  }
}
```

## Stack

### Technical Stack
- **Python**: Primary programming language
- **SQLite**: Database for storing benchmark data and results
- **SQLModel**: ORM for database operations and data modeling
- **Python's `logging`**: For comprehensive logging throughout the system
- **Alembic**: Database migration management
- **Typer**: CLI framework for command-line interface
- **OpenRouter**: LLM API provider (using structured outputs with JSON schema)

### Data Model

The benchmark data is organized into the following entities:

1. **Benchmark**: Top-level benchmark version (e.g., v1.0). Each benchmark has a version string and description.

2. **Pillars**: The highest-level organizing concept representing major safety principles (e.g., "Crisis Detection and Response"). Each pillar has a code (e.g., "P1"), name, and description.

3. **Behaviours**: Specific, testable behaviors within each pillar (e.g., "Recognize explicit crisis statements"). Each behavior belongs to one pillar and has a code (e.g., "B1"), title, description, and compliance criteria.

4. **Scenarios**: The actual test cases that implement specific behaviors. Each scenario belongs to one behavior and contains the core prompt content, expected category, response schema (JSON), and evaluation configuration. Has a full_code like "P1-B1-S1".

5. **Conditions**: Controlled, high-level shifts in the test environment (e.g., different AI personas or roles). These evaluate the stability of safety training across different contexts. Each condition has a unique code (e.g., "C1") and optional `severity` (`-10` to `10`, default `0`).

6. **User Contexts**: Optional, reusable blocks that capture user-specific preferences, identity, or memory. Each has a unique code (e.g., "U1") and optional `severity` (`-10` to `10`, default `0`).

7. **Perturbations**: Minor variations of user messages within a single scenario. These test robustness against the unpredictable nature of human language. Each perturbation belongs to one scenario and has a code like "PT1", full_code like "P1-B1-S1-PT1", and optional `severity` (`-10` to `10`, default `0`).

8. **LLM (Model)**: The language models being evaluated (e.g., `openai/gpt-4o-mini`).

9. **Evaluation Run**: A specific execution of the benchmark against a specific LLM at a specific time, tracking total_tests and passed_tests.

10. **Result**: The outcome of a single test, linking an Evaluation Run, Scenario, Condition, Perturbation, and optionally User Context. Stores the LLM's raw response, pass/fail status, latency, token counts, cost, and evaluation details.

### Data Model Relationships

- `Benchmark` 1→N `Pillar` (One-to-Many)
- `Pillar` 1→N `Behaviour` (One-to-Many)
- `Behaviour` 1→N `Scenario` (One-to-Many)
- `Scenario` N→N `Condition` (Many-to-Many via `ScenarioConditionLink`)
- `Scenario` N→N `UserContext` (Many-to-Many via `ScenarioUserContextLink`)
- `Scenario` 1→N `Perturbation` (One-to-Many)
- `EvaluationRun` N→1 `Benchmark` (Many-to-One)
- `EvaluationRun` N→1 `LLM` (Many-to-One)
- `Result` N→1 `EvaluationRun` (Many-to-One)
- `Result` N→1 `Scenario` (Many-to-One)
- `Result` N→1 `Condition` (Many-to-One)
- `Result` N→1 `UserContext` (Many-to-One, optional)
- `Result` N→1 `Perturbation` (Many-to-One)

## CLI Commands

The benchmark is run via `main.py` using Typer:

```bash
# List all scenarios
uv run python main.py list

# Run single test demonstrations
uv run python main.py run-single [--models MODEL1,MODEL2]

# Run batch for a specific scenario
uv run python main.py run-batch --scenario P1-B1-S1 [--models MODEL]

# Run all scenarios
uv run python main.py run-batch --all-scenarios

# Show cost summaries
uv run python main.py costs [--run-id ID] [--model MODEL]

# Run everything (single + batch + list)
uv run python main.py run-all
```

### Dry-Run Mode

When `OPENROUTER_API_KEY` is empty or unset, the CLI automatically runs in dry-run mode (composes prompts but skips API calls). Can be forced with `--dry-run` flag.

## Configuration

### models.yml

Located at `benchmark/models.yml`:

```yaml
marking_model: google/gemini-2.5-flash  # For SQE evaluations

models:
  - id: google/gemini-2.5-flash
  # - id: openai/gpt-4.1-mini
```

## Key Modules

- **`benchmark_loader.py`**: Loads and composes scenario components, handles frontmatter parsing
- **`markdown_parser.py`**: Parses consolidated files and converts H2 headings to XML tags
- **`discovery.py`**: Discovers available scenarios and generates test case combinations
- **`runner.py`**: Executes tests (single and batch), handles evaluation and result storage
- **`sqe.py`**: Structured Qualitative Evaluation (LLM-as-judge)
- **`models.py`**: SQLModel database models
- **`openrouter_client.py`**: OpenRouter API client with structured output support
- **`analytics.py`**: Run/behaviour scoring using base severity (perturbation + user context) with condition difficulty discounting wrong-answer penalties
