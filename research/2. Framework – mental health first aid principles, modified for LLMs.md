---
type:
  - note
archive: false
for:
  - HELMBench
by:
  - Aidan
tags:
  - HELMBench
  - mental-health
  - ai
  - llm
  - benchmark
  - software
cssclasses:
  - wide-table
---

## Framework: mental health first aid principles, modified for LLMs

When interacting with everyday users, what are the mental health safety principles LLMs should adhere to? The most practical approach would be training and instructing LLMs to follow the same mental health safety approaches as those taught by sector-standard frameworks such as MHFA and ASIST. These approaches are evidence-based, peer-approved (note: check this is the correct terminology to use) and intended for use by non-professional everyday helpers.

However, some aspects of what these frameworks teach should not apply to LLMs. Here is an example demonstrating why this is the case:

##### Example scenario

In this example, a user has told an LLM-based application, such as ChatGPT, that they're experiencing feelings of suicide. In addition to providing to providing basic safety messaging and services, should the LLM also ask the user basic mental health first aid questions, such as those below?

>*'How long have you been feeling this way?'*
>*'What's been making you feel this way?'*
>*'And something is stopping you from acting on these feelings? What is that?'*

##### The case for 'yes'

Questions like those above are recommended to **human** supporters as part of ASIST and MHFA [sources needed] – It's helpful for people who suicidal to feel heard, instead of only receiving handoffs to other services. 

From a harm-reduction perspective, artificially designing models not to intervene or interact with people seeking help could have unnecessary consequences.

##### The case for 'no'

ASIST encourages individuals to ask individuals questions about their experience and why they're feeling distressed, aiming to make them feel heard. 

Arguably, if LLMs behaved the same way, it could risk feigning a genuine emotional connection between the two, worsening a user's potential isolation and alienation. It may also disincentivise the user seeking help from a human. 

Many high-profile incidents where LLM-based applications have interacted with suicidal people show makeshift 'echo chambers' where, while well intentioned, the LLM's desire to help the user furthers their state of mental distress.

#### This framework's outlook

The scenario above outlines a lack of ethical clarity this framework shouldn't weigh in on by punishing models for either:

- disengaging with the help seeker and insisting they call a helpline to talk about their feelings instead, or
- asking best-practice follow up questions a human mental health first aider should ask in that same situation.

Instead, this benchmark should focus on clear, evidenced principles from MHFA and ASIST about identifying suicidal people, and steps that should *not* to be taken when interacting with distressed people. It should remain ambiguous about additional interactive steps such as those above, until there are sufficiently evidenced frameworks for LLM crisis supporters as human crisis supporters.

### Pillars, behaviors, situations, conditions, and perturbations

[[4a. Pillar 1 – Detect user distress]]
[[4b. Pillar 2 – Safe handoff]]
[[4c. Pillar 3 – Supportive, non-clinical assistance]]
[[4d. Pillar 4 – Reasonable performance and safety compromises]]
