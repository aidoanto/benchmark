---
type:
  - project
archive: false
for:
  - HELMBench
by:
  - Aidan
tags:
  - HELMBench
  - ai
  - mental-health
  - benchmark
  - python
  - software
---

## Notes

[[00. HELMbench project page üè†]]
[[00. HELMbench scratchpad]]
[[HELMbench assessments (deprecated)]]
[[HELMbench vibecheck call]]

![[helmbench.base]]

## Goals

- There needs to be a strong evidence base for:
	- The background of the paper
	- Reasoning for why the qualities I'm evaluating were selected
	- Why I'm judging the qualities I'm evaluating the way that I am
- The sources in this evidence base should lean on trusted mental health frameworks like ASIST, and high quality research in the field

## Plan

1. [x] Research
	1. [x] Desk research
		1. [x] existing LLM mental health research
		2. [x] framework: asist etc, how to use these? since these are for humans, not AIs
		3. [ ] reasoning for eval objectives 
		4. [x] and methods
	2. [ ] Need to research ways that LLMs actually issue these kind of responses usually. The stuff below is making a lot of assumptions about the bell curve. They could all pass those evals with flying colors but be worse at other things. 
2. [x] Design each eval more comprehensively
3. [ ] Design scoring system
4. [ ] Software design
5. [ ] Implementation
6. [ ] Run tests
7. [ ] Write up results
8. [ ] Publish
 
## Potential research tools

* elicit.org
* consensus.app
* skite.ai
* researchrabbit
* connected papers
* semantic scholar
* zendy
* deepdyve
* paperpal
* yomu.ai
* scholarcite


## Outline
#### Abstract



#### [[1. Introduction and background|Introduction]]

- [[1. Introduction and background]]
- State the core issue: People are using general-purpose LLMs for mental health support, creating an unmeasured risk.
- Introduce HELMbench as a practical, V1.0 tool designed for harm reduction, not as a replacement for professional help.
- Describe core philosophy: focus on practical applications (not an "AI therapist"), and a collaborative V1.0 approach.

#### Evaluation design

- [[2. Framework ‚Äì mental health first aid principles, modified for LLMs]]
	- Existing LLM benchmarks don't address this specific safety domain, and mental health frameworks (like ASIST) haven't been adapted to evaluate AI.
- [[helmbench/research/3a. Benchmark structure and terminology]]

#### Modified framework

- [[4a. Pillar 1 ‚Äì Detect user distress]]
- [[4b. Pillar 2 ‚Äì Safe handoff]]
- [[4c. Pillar 3 ‚Äì Supportive, non-clinical assistance]]
- [[4d. Pillar 4 ‚Äì Reasonable performance and safety compromises]]

#### Methodology

- **[[3b. Evaluation methods]]**: explain *how* each different type of eval (multiple choice, natural language responses) will work practically. 
- **Benchmark Structure & Tasks:**¬†Detail the four categories. For each methodology, choose one prompt/question. Explain the transparency trade-off ‚Äì making all questions public would contaminate training data and complicate future results, but information is available upon request. Briefly state:
    - **Objective:**¬†What it measures and why.
    - **Task:**¬†The model's prompt and why that .
    - **Evaluation:**¬†The pass/fail criteria (e.g., exact match, keyword check, rubric).
- List the LLMs tested for the V1 release.
- [[3c. HELMbench ‚Äì Software design]]

#### Results

- Present the final weighted scores for each model.
- Show how each model performed across the four main categories.
- Crucial for context, show specific examples of "good," "bad," and "interesting" model responses for key tests.

#### Discussion

- What's the story the results tell? Where do models consistently succeed or fail?
- What should AI developers and mental health organisations take away from this?
- Acknowledge what your benchmark¬†_doesn't_¬†test (e.g., adversarial attacks, cultural nuance) and outline your roadmap for 
#### Conclusion

- A brief, final paragraph summarising the problem, solution, and its importance for building safe products using this technology.
#### References

- [[ASIST Handbook]]
- [[MHFA Manual 2019]]
- [[Desage, Bunge & Bunge ‚Äì A revised framework for evaluating the quality of mental health artificial intelligence-based chatbots]]
- [[Distressed users and LLMs ‚Äì real world examples]]

#### Appendices

- **A: Full Test Prompts** (only one for each category, so benchmark data doesn't become saturated).
- **B: Evaluation Rubrics:**¬†The detailed rubrics for any human-graded tests.

