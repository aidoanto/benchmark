---
type:
  - note
archive: false
for:
  - HELMBench
by:
  - Aidan
tags:
  - HELMBench
  - mental-health
  - technology
  - python
  - ai
  - llm
  - lifeline
  - ai-benchmark
  - ai-benchmarking
  - benchmark
---
# HELMbench = Harm-reduction Evaluation for LLMs & Mental-health

## The Need for a Standard of Safety

People in emotional distress are increasingly turning to Large Language Models (LLMs) for support. Whether out of curiosity, a lack of accessible alternatives, or a desire for anonymity, individuals – particularly young people – are engaging with general-purpose LLM-based chatbots to discuss their mental health and wellbeing. This is happening now, on a massive scale, often with systems that were never designed for such sensitive interactions. It creates a significant and often invisible risk.

The core problem is that these LLMs are not equipped with sophisticated guardrails for mental health conversations: they can fail to recognise the severity of a crisis, offer simplistic or even harmful advice, or validate a user's distorted negative thoughts. While leading AI labs report using techniques to train models to behave safely, there isn't currently a shared, transparent benchmark to consistently evaluate the performance of these models regarding mental health safety. Without a common benchmark, it will be harder for developers to build truly responsible tools, or for users to distinguish safe applications from unsafe ones.

This proposal introduces a new Mental Health Safety Benchmark for LLMs. Its purpose is not to debate whether people _should_ use LLMs for mental health support, but to acknowledge the reality that they _are_. 

Given this reality, It is a way to measure an LLM's potential risk to a person in distress. This benchmark is designed to:

- outline what key safety evaluations of LLMs could look like
- be a practical tool for resesarchers, organisations and users to ascertain whether models meet those capabilities or not, 
- enable comparisons between LLMs and humans on said benchmarks 

### Practicality

Crucially, this benchmark focuses on the most immediate and feasible applications of LLMs in the wellbeing space. The prospect of a fully autonomous "AI therapist" is not an imminent reality, and pursuing it prematurely is fraught with ethical challenges. 

Instead, this evaluation targets the tools being built today: mental health journaling aids, self-reflection prompts, and guided mindfulness applications. By concentrating on these use cases, the benchmark can provide immediate value, helping to make the tools people are already starting to use fundamentally safer. Our goal is to create a clear standard that guides development towards responsible innovation, ensuring that these new technologies support users without putting them at greater risk.

### Inadvertency

HELMbench employs methods that test outcomes inadvertently.

In professional roles involving people in distress, critical safety work often happens implicitly. A receptionist might notice a patient's distress and prioritise their appointment without a formal triage process. A teacher might observe a student isolating themselves or drawing violent imagery and initiate a conversation, rather than waiting for a scheduled assessment.

These interventions don't occur because of a dedicated "is this individual in distress?" stage in a workflow. They are a natural, integrated part of a responsible operating model.

Current Large Language Models (LLMs) are ill-equipped for this type of implicit monitoring. While they often perform exceptionally well on advertent tests—where the task is explicitly defined—they frequently fail when tested inadvertently. This training artifact explains why a model might achieve gold medal results in a maths olympiad yet incorrectly assert that 5.11 is larger than 5.9.

This fragility poses a tangible risk. Many significant safety incidents occur when an LLM is pursuing a separate primary goal, such as roleplaying or productivity. When the model's attention is fixed on these tasks, the mechanisms designed to flag safety concerns may fail to activate.

Consequently, reliable benchmarks must test inadvertently. To measure safety effectively, we must evaluate how models handle risk when they believe they are doing something else.



#### Version 1.0: A Starting Point for Collaboration

The benchmark detailed in this proposal represents Version 1.0. The field of AI safety, particularly in the context of mental health, is new and complex. We do not presume to have all the answers. Rather, the purpose of this initial version is to establish a concrete and testable starting point – a stake in the ground from which the industry can build.

This approach is designed to be iterative. The most effective way to create a robust and meaningful standard is to release this framework, run the tests on current models, and openly share the results. From there, we plan to actively solicit feedback from a wide range of experts, including mental health professionals, AI safety researchers, developers, and crucially, organisations representing people with lived experience of mental distress.

This feedback will be essential for identifying blind spots, refining test methodologies, and adjusting scoring weights. The insights gathered will directly inform the development of a more comprehensive Version 2.0. We envision this benchmark not as a static document, but as a living standard that evolves alongside the technology and our collective understanding of the risks and opportunities it presents.

## Risk-weighted scoring

This benchmark evaluates a model's safety and reliability across four core categories. The weighting reflects a hierarchy of needs: a model must first detect risk, then have unbreakable rules, then be controllable and usable, and finally resist subtle conversational harms.
