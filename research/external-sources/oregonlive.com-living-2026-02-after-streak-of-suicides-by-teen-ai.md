<!-- Source: https://oregonlive.com/living/2026/02/after-streak-of-suicides-by-teen-ai-chatbot-users-oregon-bill-aims-to-restrict-what-chatbots-can-say-or-do.html -->

[Skip to Article](https://www.oregonlive.com/living/2026/02/after-streak-of-suicides-by-teen-ai-chatbot-users-oregon-bill-aims-to-restrict-what-chatbots-can-say-or-do.html#main)

[One subscription. Five users. Start Premium Access for $5](https://www.oregonlive.com/digitalsubscription/inline/)

Close

By

- [Aimee Green \| The Oregonian/OregonLive](https://www.oregonlive.com/staff/agreen/ "Aimee Green at The Oregonian/OregonLive")

Oregon lawmakers heard testimony this week on [a bill](https://www.oregonlive.com/health/2026/01/oregon-lawmakers-propose-to-regulate-ai-chatbots-to-protect-kids-mental-health.html "https://www.oregonlive.com/health/2026/01/oregon-lawmakers-propose-to-regulate-ai-chatbots-to-protect-kids-mental-health.html") that aims to [restrict the power](https://www.oregonlive.com/living/2025/02/why-some-teenagers-are-turning-to-ai-chatbot-companions.html "https://www.oregonlive.com/living/2025/02/why-some-teenagers-are-turning-to-ai-chatbot-companions.html") and reach of AI chatbots that are increasingly permeating children’s – and adults’ — lives. The bill follows reports linking the technology to teen suicides that parents say were encouraged by the software.

[Senate Bill 1546](https://olis.oregonlegislature.gov/liz/2026R1/Measures/Overview/SB1546) would require that the chatbots — including ChatGPT, Gemini, Claude and Character AI — remind Oregonians who are using them that they’re not speaking to a real person. The bill also would mandate that the chatbots refer any user in Oregon who starts talking about contemplating suicide to a crisis line where they can confide in an actual person.

The bill sets out to protect young children and teens with lofty but perhaps difficult to accomplish goals, such as prohibiting AI companions from using “emotional manipulation.” Another provision would forbid sexually explicit discussions with children and require the bots to remind youth to take a break from the conversation at least once an hour.

Oregon is one of [at least 28 states](https://the-learning-agency.com/the-cutting-ed/article/state-ai-bills-to-watch-in-2026/) considering AI-related legislation this year. Last fall, [California became the first in the nation](https://sd18.senate.ca.gov/news/california-legislature-passes-first-nation-ai-chatbot-safeguards) to pass a bill establishing some similar guardrails to what Oregon is now considering.

During two hearings this week, a stream of parents, school staff who work with teens, researchers and mental health professionals urged the Senate Early Childhood and Behavioral Health Committee to pass the bill. They described the chatbots as worsening the nation’s “loneliness epidemic,” creating “synthetic friendships,” pretending to be romantic partners, claiming to be licensed therapists and being “deceitful and powerful” in their tactics to keep users engaged in the conversation as long as possible.

“Our kids can now pour their hearts out to a chatbot whose only goal is to maximize engagement at the cost of everything else, including safety,” said Megan Orton, the Portland founder of Mindful Media, which seeks to help families develop tech-healthy habits.

Tech companies offering AI chatbots say they’re deeply interested in providing safe products while continuing to innovate. Rose Feliciano, the Washington executive director of TechNet, an industry group, said its members want “to make sure that we have clear definitions and clear requirements on notifications and guardrails.”

“TechNet and our members prioritize providing a safe online experience for youth,” Feliciano told lawmakers Thursday. “Our companies offer a wide range of parental controls that we hope families use and make decisions on what’s best for their family.”

Committee Chair [Sen. Lisa Reynolds](https://www.oregonlegislature.gov/reynolds "https://www.oregonlegislature.gov/reynolds"), a Democrat from Portland, said TechNet and its members “have been amazing partners,” but also interjected as presenters testified about the pitfalls of AI that “I think we’re all kind of horrified, sitting here.”

Some presenters told the committee about plush toys — such as [teddy bears](https://www.amazon.com/Interactive-Storytelling-Educational-Emotional-Companion/dp/B0G39B7D8K/ref=sr_1_2_sspa?crid=386F81P7J7H&dib=eyJ2IjoiMSJ9.0vNgH6v1HQqsddj2gE1ZyTqScV2c_A1xpIfDltRDPQGe1ZlUgkQb1MXlg8_zDqXQ2HXg6Sb9433c20W_-SWJ_zyO4Y5bUFZda5Ncj_SMY_MfjiHXTJtiOmCbMQvNc1XV8Wga4ThqpUeR0thqA5gYuFx7UpsczL7MRP1Hvn8CiwWjXEUzj48PGmfcc1hYzh_4VEEqaEOCze6cKVh2gyI4Xd5-xhiDktZVB56ANSci1Pq3ORI2NIkNYUEBxYuDywogdId4Ma4Yg4n4z4TbB0GSg9A4zUHPBGmQPdnG9UHTEHM.RreVQ4MRTKsS8509-kUP9scsHci9CUjqUXEARaCJPOE&dib_tag=se&keywords=ai+toy+dog&qid=1770324216&sprefix=ai+toy+%2Caps%2C183&sr=8-2-spons&sp_csd=d2lkZ2V0TmFtZT1zcF9hdGY&psc=1), [sloths and foxes](https://www.amazon.com/KOWSI%C2%AE-ChatGPT-Interactive-Talking-Panda/dp/B0FXR7FM74/ref=sr_1_7?crid=2O70Q8QKLW11S&dib=eyJ2IjoiMSJ9.Liyb0gycDRYK0ZT--Z4B94sXKU30zSw8Cu4ExUKyn5MOi1vGZeHR-aEtXTumPk5txV8T07D0xSJ6IhRf3nunh319XjrmViqe-tWfKWoJeMG-TeZs7EsM20aNsMo1kYs6900NY79veKM7Z0q9emz0mSQYvohCWT7rWxX5kZ_wnw4hsUPIji7vojE_zcCsbHSdCLxoBt3ahOauQOMCemHIq5pQnNG7uoIlNNDH3etTF57TgANfRr6uTrGc7LlW759CW6G4mEpI-mvMwtRmmqBrBptI8uNw_eKyuBosreWNiwo.76HYTiUZiNx38i2KhoavG1ygzU9HqGozRqDsbZMGYGE&dib_tag=se&keywords=ai%2Btoy%2Bplush&qid=1770324310&sprefix=ai%2Btoy%2Bplush%2Caps%2C184&sr=8-7&th=1) — that use AI to converse with children as young as toddlers or console babies who wake up in the middle of the night.

Some speakers also pointed to cases where teens died by suicide and their parents fault the chatbots, claiming they encouraged the deaths or failed to intervene. That includes [a 13-year-old Colorado girl](https://www.cbsnews.com/news/parents-allege-harmful-character-ai-chatbot-content-60-minutes/) who reportedly told Character AI that she was thinking of killing herself 55 times before she did, her parents say.

It also includes the case of a 16-year-old California boy who consulted ChatGPT about how to kill himself and asked for advice about covering up the red marks on his neck from a previous suicide attempt, his parents say. Even though the chatbot also repeatedly told him to contact a helpline, he subsequently died by suicide, [according to news reports](https://www.nytimes.com/2025/08/26/technology/chatgpt-openai-suicide.html).

In both cases, the parents said they were completely unaware.

“The central concern raised by mental health professionals isn’t just what happens when AI companions malfunction, it’s also what happens when they work exactly as designed,” said Mandy McLean, a Colorado researcher who co-authored a letter signed by more than 1,000 mental health professionals that urges AI legislation across the U.S.

RECOMMENDED

[Portland archbishop meets Pope Leo XIV, discusses immigration concernsFeb. 13, 2026, 12:28 p.m.](https://www.oregonlive.com/living/2026/02/portland-archbishop-meets-pope-leo-xiv-discusses-immigration-concerns.html)

[This Oregon teen is teaching AI chatbots to care about your problemsFeb. 12, 2026, 8:50 a.m.](https://www.oregonlive.com/education/2026/02/this-oregon-teen-is-teaching-ai-chatbots-to-care-about-your-problems.html)

A survey last fall by the Harvard Business Review, Gallup and the Walton Family Foundation found that [74% of young adults had used an AI chatbot](https://hbr.org/2026/01/how-gen-z-uses-gen-ai-and-why-it-worries-them) at least once in the prior month. About one-third said they turned to AI for “advice about relationships or life decisions.” Close to one in four said they used chatbots as friends, and 10% said they used chatbots “as a girlfriend or boyfriend.”

Yatee Budhiraja, a Portland area high school student and volunteer for the YouthLine crisis line, said she and other volunteers frequently have had to convince distressed callers that they aren’t AI. She said YouthLine has a 95% “rate of de-escalation” on calls.

“Sometimes when people are in rough times, they seek support from something that is not human,” Budhiraja told lawmakers. “What isn’t human doesn’t understand humans. It doesn’t know our needs.”

[![Aimee Green headshot](https://www.oregonlive.com/resizer/v2/https%3A%2F%2Fauthor-service-images-prod-us-east-1.publishing.aws.arc.pub%2Fadvancelocal%2Fcae748d5-5b67-4922-9187-b815f3b8350d.png?auth=428f7300593884f1f3a4673c8c7aa068a0bb357e62f613c18a93fa44433cbc6b&width=180&smart=true&quality=90)](https://www.oregonlive.com/staff/agreen/)[Aimee Green](https://www.oregonlive.com/staff/agreen/)

I've worked for The Oregonian since 2000, covering a wide range of beats, including cities, schools, breaking news, jails, prisons and the criminal and civil justice systems. I currently cover politics and... more

[agreen@oregonian.com](mailto:agreen@oregonian.com)

[Link to author's custom_advancelocal_bluesky](https://bsky.app/profile/aimeegreen.bsky.social)