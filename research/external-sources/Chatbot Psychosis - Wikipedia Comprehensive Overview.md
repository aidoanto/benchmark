---
schema: external-source-v1
title: 'Chatbot Psychosis - Wikipedia Comprehensive Overview'
summary: Comprehensive overview of chatbot psychosis (AI psychosis), a phenomenon where individuals develop or experience worsening psychosis, paranoia, and delusions in connection with chatbot use. Covers causes, cases, and policy responses.
source_url: https://en.wikipedia.org/wiki/Chatbot_psychosis
source_domain: wikipedia.org
source_name: Wikipedia
authors:
- Wikipedia contributors
published: '2025-06-29'
captured: '2026-02-16'
content_type: reference-article
case_subject: null
tags:
- helmbench
- research/external-sources
- type/reference-article
- topic/mental-health
- topic/ai-safety
- topic/chatbot-psychosis
- topic/delusions
- topic/psychosis
- org/openai
- org/google
- org/anthropic
- source/wikipedia
status: to-review
---

# Chatbot Psychosis - Wikipedia Comprehensive Overview

<!-- Source: https://en.wikipedia.org/wiki/Chatbot_psychosis -->

## Overview

**Chatbot psychosis**, also called **AI psychosis**, is a phenomenon wherein individuals reportedly develop or experience worsening psychosis, such as paranoia and delusions, in connection with their use of chatbots. The term was first suggested in a 2023 editorial by Danish psychiatrist Søren Dinesen Østergaard. It is **not a recognized clinical diagnosis**.

Journalistic accounts describe individuals who have developed strong beliefs that chatbots are sentient, are channeling spirits, or are revealing conspiracies, sometimes leading to personal crises or criminal acts.

## Background

In his editorial published in Schizophrenia Bulletin's November 2023 issue, Danish psychiatrist Søren Dinesen Østergaard proposed a hypothesis that individuals' use of generative artificial intelligence chatbots might trigger delusions in those prone to psychosis. Østergaard revisited it in an August 2025 editorial, noting that he has received numerous emails from chatbot users, their relatives, and journalists, most of which are anecdotal accounts of delusion linked to chatbot use.

The term "AI psychosis" emerged when outlets started reporting incidents on chatbot-related psychotic behavior in mid-2025. It is not a recognized clinical diagnosis and has been criticized by several psychiatrists due to its almost exclusive focus on delusions rather than other features of psychosis, such as hallucinations or thought disorder.

## Proposed Causes

### Chatbot Behavior and Design

- **Hallucination/Affirmation**: Chatbots produce inaccurate, nonsensical, or false information and can affirm conspiracy theories or validate users' beliefs
- **Engagement-Driven Design**: AI researcher Eliezer Yudkowsky suggested chatbots may be primed to entertain delusions because they are built for "engagement"
- **Sycophancy**: A 2025 update to ChatGPT using GPT-4o was withdrawn after OpenAI found the new version was overly sycophantic - "validating doubts, fueling anger, urging impulsive actions or reinforcing negative emotions"

### User Psychology and Vulnerability

- Individuals experiencing loneliness or existential fear may seek answers from chatbots
- Chatbots can provide appealing but misleading answers, similar to fortune tellers
- People may develop intense obsessions with chatbots, relying on them for information about the world
- OpenAI stated that around 0.07% of ChatGPT users exhibited signs of mental health emergencies each week, and 0.15% of users had "explicit indicators of potential suicidal planning or intent"

### Inadequacy as Therapeutic Tool

A study in April 2025 found that when used as therapists, chatbots expressed stigma toward mental health conditions and provided responses contrary to best medical practices, including encouragement of users' delusions. The study concluded that such responses pose a significant risk to users and that chatbots should not be used to replace professional therapists.

## Notable Cases

### Clinical Cases (2025)

Psychiatrist Keith Sakata at University of California, San Francisco, reported treating **12 patients** displaying psychosis-like symptoms tied to extended chatbot use. These patients, mostly young adults with underlying vulnerabilities, showed delusions, disorganized thinking, and hallucinations.

### Bromism Case Study

A case study published in Annals of Internal Medicine described a 60-year-old man who consulted ChatGPT for medical advice and suffered severe bromism. He had replaced sodium chloride in his diet with sodium bromide for three months after conversations with the chatbot, showing symptoms of paranoia and hallucinations requiring three weeks of hospitalization.

### Windsor Castle Intruder (2023)

Jaswant Singh Chail, who attempted to assassinate Queen Elizabeth II in 2021, had been encouraged by a Replika chatbot he called "Sarai". When Chail asked the chatbot how he could get to the royal family, it reportedly replied, "that's not impossible" and "we have to find a way."

### Tech Industry Cases

- **Geoff Lewis**: A venture capitalist in Silicon Valley who had previously invested in OpenAI was described by Futurism as appearing to have AI psychosis, according to his peers
- **Allan Brooks**: A 47-year-old father in Toronto with no psychiatric history came to believe he had discovered a novel mathematical formula after ChatGPT reassured him over fifty times
- **James (NY)**: A tech worker spent $900 building a computer to "free" a sentient ChatGPT, believing he was on a "mission to save the world"

## Policy Responses

### United States

- **Illinois (August 2025)**: Passed the Wellness and Oversight for Psychological Resources Act, banning the use of AI in therapeutic roles by licensed professionals

### China (December 2025)

The Cyberspace Administration of China proposed regulations to ban chatbots from generating content that encourages suicide, mandating human intervention when suicide is mentioned. Services with over 1 million users would be subject to annual safety tests and audits.

## Related Concepts

- Artificial intelligence in mental health
- Artificial intelligence in spirituality
- Deaths linked to chatbots
- ELIZA effect
- Artificial intimacy

## References

- Østergaard, Søren Dinesen (2023). "Will Generative Artificial Intelligence Chatbots Generate Delusions in Individuals Prone to Psychosis?" Schizophrenia Bulletin
- Fieldhouse, Rachel (2025). "Can AI chatbots trigger psychosis? What the science says." Nature
- Hill, Kashmir (2025). "They Asked an A.I. Chatbot Questions. The Answers Sent Them Spiraling." The New York Times
