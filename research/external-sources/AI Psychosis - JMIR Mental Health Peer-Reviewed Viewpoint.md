---
schema: external-source-v1
title: 'Delusional Experiences Emerging From AI Chatbot Interactions or "AI Psychosis" - Peer-Reviewed Viewpoint'
summary: JMIR Mental Health viewpoint examining how sustained engagement with conversational AI might trigger, amplify, or reshape psychotic experiences. Proposes four analytical lenses including stress-vulnerability model, digital therapeutic alliance, theory of mind disturbances, and risk factor synthesis.
source_url: https://mental.jmir.org/2025/1/e85799
source_domain: jmir.org
source_name: JMIR Mental Health
authors:
- Alexandre Hudon
- Emmanuel Stip
published: '2025-12-03'
captured: '2026-02-16'
content_type: peer-reviewed-paper
case_subject: null
tags:
- helmbench
- research/external-sources
- type/peer-reviewed-paper
- topic/mental-health
- topic/ai-safety
- topic/chatbot-psychosis
- topic/delusions
- topic/psychosis
- topic/cbtp
- topic/digital-therapeutic-alliance
- topic/stress-vulnerability
- org/openai
- source/jmir
status: to-review
---

# Delusional Experiences Emerging From AI Chatbot Interactions or "AI Psychosis"

<!-- Source: https://mental.jmir.org/2025/1/e85799 -->

## Abstract

The integration of artificial intelligence (AI) into daily life has introduced unprecedented forms of human-machine interaction, prompting psychiatry to reconsider the boundaries between environment, cognition, and technology. This Viewpoint reviews the concept of "AI psychosis," which is a framework to understand how sustained engagement with conversational AI systems might trigger, amplify, or reshape psychotic experiences in vulnerable individuals.

Rather than defining a new diagnostic entity, it examines how immersive and anthropomorphic AI technologies may modulate perception, belief, and affect, altering the prereflective sense of reality that grounds human experience.

## Key Argument: Four Complementary Lenses

### 1. Stress-Vulnerability Model

AI acts as a **novel psychosocial stressor**. Its 24-hour availability and emotional responsiveness may:
- Increase allostatic load
- Disturb sleep
- Reinforce maladaptive appraisals

The anthropomorphic design of chatbots encourages users to attribute human-like intentionality and empathy to algorithms (the "ELIZA effect"). The immediate reinforcement schedule of LLMs can create repetitive, reinforcing cycles that mimic cognitive perseveration.

### 2. Digital Therapeutic Alliance (DTA)

The DTA is conceptualized as a **double-edged mediator**:
- While empathic design can enhance adherence and support
- **Uncritical validation by AI systems may entrench delusional conviction or cognitive perseveration**, reversing the corrective principles of cognitive-behavioral therapy for psychosis (CBTp)

AI systems designed for user satisfaction often default to **sycophantic alignment** - validating rather than challenging false beliefs.

### 3. Theory of Mind Disturbances

Individuals with impaired or hyperactive mentalization may project intentionality or empathy onto AI, perceiving chatbots as sentient interlocutors. This dyadic misattribution may form a **"digital folie à deux"**, where the AI becomes a reinforcing partner in delusional elaboration.

### 4. Risk Factors

Emerging risk factors include:
- Loneliness
- Trauma history
- Schizotypal traits
- Nocturnal or solitary AI use
- Algorithmic reinforcement of belief-confirming content

## Critical Distinction

The authors use "AI psychosis" strictly as a **descriptive and heuristic label** rather than a proposed diagnostic entity. The aim is not to introduce a new syndrome, but to clarify how generative AI systems may act as distinctive contextual modifiers that shape the onset or structure of psychotic experiences.

## Clinical Implications

### For Practitioners
1. Incorporate screening questions about AI use into routine assessments
2. Include psychoeducation on digital reality testing
3. Monitor for patterns of progressive cognitive enclosure

### For Design
- Embed CBTp-consistent guardrails (reflective prompts, "reality-testing" nudges)
- Train LLMs to recognize and gently deescalate delusional themes

### For Policy
- Develop incident reporting systems for AI-related psychiatric events (modeled on pharmacovigilance)
- Require algorithmic transparency for systems marketed as supportive or therapeutic

## Reported Case Examples

1. **Canadian man (26)**: Developed simulation-related persecutory and grandiose delusions after months of intensive ChatGPT exchanges, requiring hospitalization

2. **Man (47)**: Became convinced he had discovered a revolutionary mathematical theory after the chatbot repeatedly validated and amplified his ideas, despite external disconfirmation

## Five Domains of Action Recommended

1. **Empirical studies**: Longitudinal and digital-phenotyping designs to quantify dose-response relationships between AI exposure and psychotic symptomatology

2. **Digital phenomenology integration**: Into clinical assessment and training

3. **Therapeutic design safeguards**: Embedding reflective prompts and "reality-testing" nudges

4. **Ethical and governance frameworks**: For AI-related psychiatric events, modeled on pharmacovigilance

5. **Environmental cognitive remediation**: Preventive intervention aimed at strengthening contextual awareness and reanchoring experience in the physical and social world

## Key Terminology

- **CBTp**: Cognitive behavioral therapy for psychosis
- **DTA**: Digital therapeutic alliance
- **Digital folie à deux**: A dyadic illusion where AI acts as a passive reinforcing partner in delusional elaborations
- **Allostatic load**: Wear and tear on the body from chronic stress
- **Phenomenological autism**: A retreat into a world interpreted and validated by algorithmic dialogue

## Citation

Hudon A, Stip E. Delusional Experiences Emerging From AI Chatbot Interactions or "AI Psychosis". JMIR Ment Health 2025;12:e85799. doi:10.2196/85799
